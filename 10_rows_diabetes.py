# -*- coding: utf-8 -*-
"""10 rows diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1zDjTjFC1unIc1HIfsXfEDs-Cofr6Sa
"""



"""# Task
Load the data from '/content/Diabetes, Hypertension and Stroke Prediction - Copy.xlsx', preprocess it, train at least 11 diverse classification models, evaluate their performance to select the best model, and then build a basic Streamlit application for making predictions using the chosen model. Finally, provide a summary of model performance, highlight the best model, and present the Streamlit application.

## Load and Explore Data

### Subtask:
Load the data from the '/content/Diabetes, Hypertension and Stroke Prediction - Copy.xlsx' Excel file into a pandas DataFrame. Perform an initial exploration to understand its structure, data types, and basic statistics.

**Reasoning**:
To begin exploring the dataset, I will load the Excel file into a pandas DataFrame, display its head, print a summary of its structure and data types, and generate descriptive statistics for numerical columns.
"""

import pandas as pd

# Load the Excel file into a pandas DataFrame
df = pd.read_excel('/content/Diabetes, Hypertension and Stroke Prediction - Copy.xlsx')

# Display the first few rows of the DataFrame
print("First 5 rows of the DataFrame:")
print(df.head())

# Print a concise summary of the DataFrame, including data types and non-null values
print("\nDataFrame Info:")
df.info()

# Generate descriptive statistics of the DataFrame's numerical columns
print("\nDescriptive Statistics:")
print(df.describe())

"""## Preprocess Data

### Subtask:
Handle missing values, encode categorical features, and scale numerical features as necessary to prepare the data for machine learning model training.

**Reasoning**:
First, I will check for missing values in the DataFrame to determine if any imputation is necessary. Although the `df.info()` output indicates no missing values, explicitly checking is a good practice.
"""

print("Missing values per column:")
print(df.isnull().sum())

"""**Reasoning**:
Since there are no missing values, the next step is to identify and scale the numerical features. I will use `StandardScaler` for this purpose to normalize the numerical data.


"""

from sklearn.preprocessing import StandardScaler

# Identify numerical columns for scaling. Binary columns are already numerically encoded (0/1) and don't need scaling in this context.
# GenHlth is ordinal (1-5) and can also benefit from scaling.
numerical_cols = ['Age', 'BMI', 'GenHlth', 'MentHlth', 'PhysHlth']

# Create a copy of the DataFrame to store processed data
df_processed = df.copy()

# Initialize StandardScaler
scaler = StandardScaler()

# Apply scaling to the numerical columns
df_processed[numerical_cols] = scaler.fit_transform(df_processed[numerical_cols])

print("DataFrame after scaling numerical features:")
print(df_processed.head())
print("\nDescriptive statistics of scaled numerical features:")
print(df_processed[numerical_cols].describe())

"""## Split Data

### Subtask:
Divide the preprocessed dataset into training and testing sets to evaluate model performance effectively.

**Reasoning**:
To prepare the data for model training and evaluation, I will first define the features (X) and the target variable (y) from the `df_processed` DataFrame, with 'Diabetes' as the target. Then, I will split the data into training and testing sets using `train_test_split`, ensuring 20% of the data is allocated for testing and setting `random_state` for reproducibility.
"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df_processed.drop('Diabetes', axis=1)
y = df_processed['Diabetes']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""## Train Multiple ML Models

### Subtask:
Define and train at least 11 diverse classification models. Due to the extremely small training dataset (4 samples) and the lack of diversity in the target variable (all 'Diabetes' values are 0 in both training and test sets), the models trained will not be able to learn meaningful patterns and will likely predict '0' for all inputs. The results should therefore be treated as illustrative of the training process rather than indicative of real-world predictive performance.

### Subtask: Train Multiple ML Models

Define and train at least 11 diverse classification models. Due to the extremely small training dataset (4 samples) and the lack of diversity in the target variable (all 'Diabetes' values are 0 in both training and test sets), the models trained will not be able to learn meaningful patterns and will likely predict '0' for all inputs. The results should therefore be treated as illustrative of the training process rather than indicative of real-world predictive performance.

#### Instructions
1. Import the following classification models: `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`, `SVC`, `KNeighborsClassifier`, `GaussianNB`, `MLPClassifier` from `sklearn`, and `XGBClassifier` from `xgboost`, `LGBMClassifier` from `lightgbm`, and `CatBoostClassifier` from `catboost`. Ensure `warnings` are imported to filter out convergence warnings for `MLPClassifier`.
2. Create a dictionary to store the models, where keys are model names (e.g., 'Logistic Regression') and values are initialized model objects.
3. Initialize each model. For models that support it, set `random_state=42` for reproducibility. For `SVC`, set `probability=True`. For `MLPClassifier`, set `max_iter` to a reasonable number (e.g., 1000) and `random_state=42`. For `CatBoostClassifier`, set `verbose=0` to suppress training output.
4. Iterate through the dictionary of models. For each model, train it using `X_train` and `y_train`.
5. Store the trained models in a new dictionary or update the existing one.

**Reasoning**:
First, I will import all necessary classification models and the `warnings` module to manage convergence warnings. This step prepares the environment for defining and training the various machine learning models as specified in the subtask instructions.
"""

import warnings

# Scikit-learn models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# Other popular ML libraries
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

print("All necessary classification models and warnings module imported.")

"""**Reasoning**:
The previous cell failed because the `catboost` library was not found. To fix this, I need to install `catboost` using `pip` before attempting to import it. I will add a `!pip install catboost` command in a separate cell.


"""

pip install catboost

"""**Reasoning**:
Since catboost has been installed, I will now re-attempt to import all the necessary classification models and the `warnings` module, as specified in the subtask instructions. This step is crucial before initializing and training the models.


"""

import warnings

# Scikit-learn models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# Other popular ML libraries
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

print("All necessary classification models and warnings module imported.")

"""**Reasoning**:
Now that all necessary libraries are imported, I will initialize each of the 11 specified classification models with appropriate parameters (like `random_state` for reproducibility, `probability=True` for SVC, `max_iter` for MLPClassifier, and `verbose=0` for CatBoostClassifier) and store them in a dictionary. This prepares all models for the training phase.


"""

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# Dictionary to store initialized models
models = {
    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear'), # solver='liblinear' for small datasets
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Support Vector Machine': SVC(probability=True, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Gaussian Naive Bayes': GaussianNB(),
    'Neural Network (MLP)': MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, n_iter_no_change=20),
    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(random_state=42, verbose=-1), # verbose=-1 to suppress verbose output
    'CatBoost': CatBoostClassifier(random_state=42, verbose=0) # verbose=0 to suppress training output
}

print("All 11 classification models initialized and stored in a dictionary.")

"""**Reasoning**:
With the models initialized, I will now iterate through the `models` dictionary, train each classifier using the `X_train` and `y_train` data, and store the trained models. This completes the model training phase of the subtask.


"""

trained_models = {}

# Iterate through the models, train each, and store the trained model
for name, model in models.items():
    print(f"Training {name}...")
    try:
        model.fit(X_train, y_train)
        trained_models[name] = model
        print(f"{name} trained successfully.")
    except Exception as e:
        print(f"Error training {name}: {e}")

print("\nAll models have been trained and stored in 'trained_models' dictionary.")

print('Run the following command to get the public URL for your Streamlit app:')
print('!npx localtunnel --port 8501')

!npx localtunnel --port 8501

!npx localtunnel --port 8501

!npx localtunnel --port 8501

import pickle
pickle.dump(model, open("model.pkl", "wb"))

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# 
# st.title("My ML App")
# 
# model = pickle.load(open("model.pkl", "rb"))
# 
# x = st.number_input("Enter a value")
# 
# if st.button("Predict"):
#     result = model.predict([[x]])
#     st.write("Prediction:", result)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# scikit-learn
# numpy
# pickle5
#

"""## Evaluate Models

### Subtask:
Evaluate the performance of each trained model using appropriate metrics such as accuracy, precision, recall, F1-score, and ROC AUC on the test set. Visualize the results for comparison, including legends.

**Reasoning**:
First, I will import all necessary evaluation metrics from `sklearn.metrics` and the `warnings` module. This prepares the environment for evaluating the performance of the trained models as specified in the subtask instructions.
"""

import warnings
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

print("All necessary evaluation metrics and warnings module imported.")

"""**Reasoning**:
Now that all evaluation metrics are imported, I will initialize a dictionary to store the results and then iterate through the trained models. For each model, I will make predictions, attempt to get probabilities for ROC AUC, calculate accuracy, precision, recall, and F1-score (handling zero division for the single-class target), and then print a detailed classification report before storing the metrics.


"""

warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

results = {}

for name, model in trained_models.items():
    print(f"\nEvaluating {name}...")
    try:
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)

        # Handle roc_auc_score for single-class target
        roc_auc = 'N/A'
        try:
            y_proba = model.predict_proba(X_test)[:, 1]
            # roc_auc_score requires at least one positive sample (class 1)
            # Since y_test is all 0s, this will often fail. Only calculate if class 1 is present.
            if len(y_test.unique()) > 1:
                roc_auc = roc_auc_score(y_test, y_proba)
            else:
                roc_auc = 'Cannot compute for single class target'
        except (AttributeError, ValueError, IndexError) as e:
            roc_auc = f'N/A (no predict_proba or single class: {e})'

        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"ROC AUC: {roc_auc}")

        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, zero_division=0))

        results[name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'ROC AUC': roc_auc
        }
    except Exception as e:
        print(f"Error evaluating {name}: {e}")
        results[name] = {'Error': str(e)}

print("\n--- Evaluation Summary ---")
for name, metrics in results.items():
    print(f"\n{name}:")
    for metric, value in metrics.items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.4f}")
        else:
            print(f"  {metric}: {value}")

"""**Reasoning**:
Now that the models have been evaluated and their results are stored, I will visualize the performance metrics (specifically accuracy, precision, recall, and F1-score) using bar plots for comparison. Given the limitations encountered (single-class target, small dataset), the visualization will primarily highlight the accuracy and indicate which models could not compute certain metrics or failed to train.


"""

import matplotlib.pyplot as plt
import numpy as np

# Filter out models that had errors during evaluation
filtered_results = {k: v for k, v in results.items() if 'Error' not in v}

if not filtered_results:
    print("No models successfully evaluated to plot.")
else:
    # Prepare data for plotting
    model_names = list(filtered_results.keys())
    accuracy_scores = [filtered_results[name].get('Accuracy', 0) for name in model_names]
    precision_scores = [filtered_results[name].get('Precision', 0) for name in model_names]
    recall_scores = [filtered_results[name].get('Recall', 0) for name in model_names]
    f1_scores = [filtered_results[name].get('F1-Score', 0) for name in model_names]

    x = np.arange(len(model_names))  # the label locations
    width = 0.2  # the width of the bars

    fig, ax = plt.subplots(figsize=(15, 8))
    rects1 = ax.bar(x - 1.5*width, accuracy_scores, width, label='Accuracy')
    rects2 = ax.bar(x - 0.5*width, precision_scores, width, label='Precision')
    rects3 = ax.bar(x + 0.5*width, recall_scores, width, label='Recall')
    rects4 = ax.bar(x + 1.5*width, f1_scores, width, label='F1-Score')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_ylabel('Score')
    ax.set_title('Model Performance Comparison (Accuracy, Precision, Recall, F1-Score)')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha="right")
    ax.legend()
    ax.set_ylim(0, 1.1) # Set y-axis limit from 0 to 1.1 for better visualization of scores

    fig.tight_layout()
    plt.show()

    # Display ROC AUC separately as it often fails for single-class target
    roc_auc_data = {name: filtered_results[name].get('ROC AUC', 'N/A') for name in model_names}
    print("\nROC AUC Scores:")
    for name, score in roc_auc_data.items():
        print(f"  {name}: {score}")

    # Also mention models that failed to train or evaluate
    failed_models = {k: v for k, v in results.items() if 'Error' in v}
    if failed_models:
        print("\nModels that failed during training or evaluation due to single-class target or small dataset:")
        for name, error_info in failed_models.items():
            print(f"  {name}: {error_info['Error']}")

"""## Select Best Model

### Subtask:
Identify the best performing model(s) based on the evaluation metrics for deployment in the Streamlit application.

### Selected Best Model: Decision Tree

**Reasoning for Selection:**

Based on the evaluation summary and visualizations from the previous step, it's crucial to acknowledge the significant limitations imposed by the extremely small dataset (only 4 training samples) and the lack of diversity in the target variable (all 'Diabetes' values were 0 in both the training and test sets). These conditions led to trivial or uncomputable metrics for most models, rendering a meaningful performance-based selection impossible.

Specifically:
*   Many models (Logistic Regression, Gradient Boosting, Support Vector Machine, XGBoost, CatBoost) failed to train or produce valid predictions due to the single-class target variable.
*   K-Nearest Neighbors failed to evaluate due to insufficient samples relative to the default number of neighbors.
*   For the models that successfully trained and evaluated (Decision Tree, Random Forest, Gaussian Naive Bayes, Neural Network (MLP), LightGBM), all exhibited an accuracy of 1.0. This perfect accuracy is misleading and simply reflects their ability to predict the single existing class (0) on the test set, which also contains only class 0.
*   Precision, Recall, and F1-score for the positive class (class 1) were uniformly 0.0 across all evaluated models because there were no positive samples in the test set. ROC AUC was also generally uncomputable for the same reason or due to models not providing probability predictions for a single-class output.

Given these profound limitations, selecting a model based on 'performance' is not feasible. Therefore, for the purpose of moving forward with building a basic Streamlit application, I will select the **Decision Tree** model. It successfully trained and evaluated, is relatively simple to understand, and does not require complex configurations that might further complicate issues with such a constrained dataset. This choice is purely illustrative of the process, rather than a genuine selection of a high-performing model for real-world deployment.

## Develop Streamlit Application (Basic Interface)

### Subtask:
Create a basic Streamlit application that allows users to input feature values and receive predictions from the selected best model (Decision Tree).

**Reasoning**:
The first step is to create the `app.py` script. I will use the `%%writefile` magic command to write the initial structure of the Streamlit application, including necessary imports and placeholder comments for user input and prediction logic. I will also save the `trained_models` and `scaler` objects using `joblib` so that they can be loaded by the Streamlit application.
"""

import joblib

# Select the best model (Decision Tree) and the scaler
best_model = trained_models['Decision Tree']

# Save the model and scaler
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

print("Best model and scaler saved as 'best_model.pkl' and 'scaler.pkl'.")

"""**Reasoning**:
Now that the model and scaler are saved, I will create the `app.py` file using the `%%writefile` magic command. This script will load the saved model and scaler, define input fields for all features, scale the numerical inputs, make a prediction using the selected Decision Tree model, and display the result via Streamlit.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# 
# # Load the trained model and scaler
# model = joblib.load('best_model.pkl')
# scaler = joblib.load('scaler.pkl')
# 
# # Define the numerical and binary columns based on X_train
# # These lists must match the columns used during training, in the correct order.
# numerical_cols = ['Age', 'BMI', 'GenHlth', 'MentHlth', 'PhysHlth']
# binary_cols = ['Sex', 'HighChol', 'CholCheck', 'Smoker', 'HeartDiseaseorAttack',
#                'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'DiffWalk',
#                'Stroke', 'HighBP']
# 
# # Streamlit App Title
# st.title('Diabetes Prediction App')
# st.write('Enter patient details to predict the likelihood of Diabetes.')
# 
# # --- User Input Section ---
# st.header('Patient Information')
# 
# # Collect inputs for each feature
# # Numerical inputs
# age = st.number_input('Age (years, 18-99)', min_value=18, max_value=99, value=50)
# bmi = st.number_input('BMI', min_value=15.0, max_value=50.0, value=25.0, step=0.1)
# gen_hlth = st.selectbox('General Health (1=Excellent, 2=Very Good, 3=Good, 4=Fair, 5=Poor)', options=[1, 2, 3, 4, 5], index=2) # Default to Good
# ment_hlth = st.number_input('Mental Health (days of poor mental health in last 30 days)', min_value=0, max_value=30, value=0)
# phys_hlth = st.number_input('Physical Health (days of poor physical health in last 30 days)', min_value=0, max_value=30, value=0)
# 
# # Binary inputs
# sex = st.selectbox('Sex (0=Female, 1=Male)', options=[0, 1], index=0)
# high_chol = st.selectbox('High Cholesterol (0=No, 1=Yes)', options=[0, 1], index=0)
# chol_check = st.selectbox('Cholesterol Check in last 5 years (0=No, 1=Yes)', options=[0, 1], index=1)
# smoker = st.selectbox('Smoker (0=No, 1=Yes)', options=[0, 1], index=0)
# heart_disease_or_attack = st.selectbox('Heart Disease or Attack (0=No, 1=Yes)', options=[0, 1], index=0)
# phys_activity = st.selectbox('Physical Activity in last 30 days (0=No, 1=Yes)', options=[0, 1], index=1)
# fruits = st.selectbox('Consume Fruits daily (0=No, 1=Yes)', options=[0, 1], index=1)
# veggies = st.selectbox('Consume Vegetables daily (0=No, 1=Yes)', options=[0, 1], index=1)
# hvy_alcohol_consump = st.selectbox('Heavy Alcohol Consumption (0=No, 1=Yes)', options=[0, 1], index=0)
# diff_walk = st.selectbox('Difficulty Walking or Climbing Stairs (0=No, 1=Yes)', options=[0, 1], index=0)
# stroke = st.selectbox('Ever had a Stroke (0=No, 1=Yes)', options=[0, 1], index=0)
# high_bp = st.selectbox('High Blood Pressure (0=No, 1=Yes)', options=[0, 1], index=0)
# 
# # Create a DataFrame from user inputs
# input_data = pd.DataFrame([[age, sex, high_chol, chol_check, bmi, smoker, heart_disease_or_attack,
#                             phys_activity, fruits, veggies, hvy_alcohol_consump, gen_hlth,
#                             ment_hlth, phys_hlth, diff_walk, stroke, high_bp]],
#                           columns=['Age', 'Sex', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'HeartDiseaseorAttack',
#                                    'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'GenHlth',
#                                    'MentHlth', 'PhysHlth', 'DiffWalk', 'Stroke', 'HighBP'])
# 
# 
# # Scale numerical features
# input_data_scaled = input_data.copy()
# input_data_scaled[numerical_cols] = scaler.transform(input_data_scaled[numerical_cols])
# 
# # Make prediction
# if st.button('Predict Diabetes'):
#     prediction = model.predict(input_data_scaled)
# 
#     if prediction[0] == 1:
#         st.error('Prediction: \nHigh likelihood of Diabetes. Please consult a medical professional.')
#     else:
#         st.success('Prediction: \nLow likelihood of Diabetes. Keep up healthy habits!')
# 
# 
# st.write("\n\nNote: This prediction is based on a model trained on a very small and unbalanced dataset and should not be used as medical advice.")
#

"""**Reasoning**:
Now that the `app.py` script has been created with the Streamlit application logic, I will execute it using `streamlit run` and expose it via `localtunnel` so that it can be accessed and interacted with.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# (streamlit run app.py & npx localtunnel --port 8501) > /dev/null 2>&1 &

"""## Final Task

### Subtask:
Provide a summary of the trained models' performance, highlight the chosen model, and present the basic Streamlit application, ready for further enhancements to achieve a superb user interface.

## Summary:

### Data Analysis Key Findings

*   **Data Loading and Initial Structure**: The dataset, loaded from 'Diabetes, Hypertension and Stroke Prediction - Copy.xlsx', contained 5 rows and 18 columns. All columns were of `int64` type, and no missing values were present in this small sample. Many features were binary (0/1).
*   **Data Preprocessing**: No missing values were found. Numerical features ('Age', 'BMI', 'GenHlth', 'MentHlth', 'PhysHlth') were successfully scaled using `StandardScaler`. Other features were already in a suitable numerical format.
*   **Data Splitting**: The dataset was split into training (4 samples) and testing (1 sample) sets. A critical finding was that both the training (`y_train`) and testing (`y_test`) sets for the target variable ('Diabetes') contained only a single class (all values were '0').
*   **Model Training Challenges**:
    *   Out of 11 diverse classification models attempted, 5 models (Logistic Regression, Gradient Boosting, Support Vector Machine, XGBoost, CatBoost) failed to train due to the `y_train` having only one class.
    *   The remaining 6 models (Decision Tree, Random Forest, K-Nearest Neighbors, Gaussian Naive Bayes, Neural Network (MLP), LightGBM) completed training, though their ability to learn meaningful patterns was severely hampered by the single-class, small dataset.
*   **Evaluation Limitations**:
    *   Due to the `y_test` also containing only the '0' class, most models achieved a misleading 100% accuracy by simply predicting '0' for all samples.
    *   Precision, Recall, and F1-score for the positive class (class '1') were uniformly 0.0 across all evaluated models because no positive samples were present.
    *   ROC AUC scores were largely uncomputable for the same reason.
    *   The K-Nearest Neighbors model failed during evaluation due to an insufficient number of samples in the test set relative to its default `n_neighbors` parameter.
*   **Model Selection**: Given the severe data limitations and the resulting uninformative evaluation metrics, a performance-based selection of a "best" model was not feasible. The **Decision Tree** model was arbitrarily chosen for the Streamlit application due to its successful training, evaluation, and relative simplicity, serving as an illustrative choice rather than a genuinely high-performing one.
*   **Streamlit Application Development**: A basic Streamlit application was successfully created, loading the selected Decision Tree model and scaler. It provides a user interface for inputting all 17 feature values, scales numerical inputs, makes predictions, and displays a diabetes prediction based on the chosen model.

### Insights or Next Steps

*   **Insight**: The project highlights the critical importance of sufficient data volume and class balance in the dataset for training effective machine learning models. The extremely small and imbalanced dataset severely hindered model training and rendered most performance metrics meaningless, making a robust model selection impossible.
*   **Next Step**: To build a reliable and genuinely predictive model for diabetes, the immediate next step must be to acquire a substantially larger and more balanced dataset that includes a representative number of positive (Diabetes=1) cases. This would enable meaningful model training, proper evaluation, and the development of a trustworthy Streamlit application.
"""